{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfa35fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:51:37.178739: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-23 15:51:37.295463: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "# solve the problem of \"libdevice not found at ./libdevice.10.bc\"\n",
    "os.environ['XLA_FLAGS'] = '--xla_gpu_cuda_data_dir=/home/r10222035/.conda/envs/tf2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17b0772",
   "metadata": {},
   "source": [
    "# Sampling datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9fcd7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mix_sample_from(npy_dirs: list, nevents: tuple, ratios=(0.8, 0.2), seed=0):\n",
    "    # npy_dirs: list of npy directories\n",
    "    # nevents: tuple of (n_VBF_SR, n_VBF_BR, n_GGF_SR, n_GGF_BR)\n",
    "    # ratios: tuple of (r_train, r_val)\n",
    "    data_tr, data_vl, data_te = None, None, None\n",
    "    label_tr, label_vl, label_te = None, None, None\n",
    "\n",
    "    npy_dir0 = Path(npy_dirs[0])\n",
    "\n",
    "    data_VBF_SR = np.load(npy_dir0 / 'VBF_in_SR-data.npy')\n",
    "    data_VBF_BR = np.load(npy_dir0 / 'VBF_in_BR-data.npy')\n",
    "    data_GGF_SR = np.load(npy_dir0 / 'GGF_in_SR-data.npy')\n",
    "    data_GGF_BR = np.load(npy_dir0 / 'GGF_in_BR-data.npy')\n",
    "\n",
    "    n_VBF_SR, n_GGF_SR, n_VBF_BR, n_GGF_BR = nevents\n",
    "    n_test = 10000\n",
    "    n_VBF_SR_test = int(data_VBF_SR.shape[0] / (data_VBF_SR.shape[0] + data_VBF_BR.shape[0]) * n_test)\n",
    "    n_VBF_BR_test = n_test - n_VBF_SR_test\n",
    "    n_GGF_SR_test = int(data_GGF_SR.shape[0] / (data_GGF_SR.shape[0] + data_GGF_BR.shape[0]) * n_test)\n",
    "    n_GGF_BR_test = n_test - n_GGF_SR_test\n",
    "\n",
    "    r_tr, r_vl = ratios\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    print(data_GGF_SR.shape[0], data_GGF_BR.shape[0], data_VBF_SR.shape[0], data_VBF_BR.shape[0])\n",
    "    idx_VBF_SR = np.random.choice(data_VBF_SR.shape[0], n_VBF_SR + n_VBF_SR_test, replace=False)\n",
    "    idx_VBF_BR = np.random.choice(data_VBF_BR.shape[0], n_VBF_BR + n_VBF_BR_test, replace=False)\n",
    "    idx_GGF_SR = np.random.choice(data_GGF_SR.shape[0], n_GGF_SR + n_GGF_SR_test, replace=False)\n",
    "    idx_GGF_BR = np.random.choice(data_GGF_BR.shape[0], n_GGF_BR + n_GGF_BR_test, replace=False)\n",
    "\n",
    "    idx_VBF_SR_tr = idx_VBF_SR[:int(n_VBF_SR*r_tr)]\n",
    "    idx_VBF_BR_tr = idx_VBF_BR[:int(n_VBF_BR*r_tr)]\n",
    "    idx_GGF_SR_tr = idx_GGF_SR[:int(n_GGF_SR*r_tr)]\n",
    "    idx_GGF_BR_tr = idx_GGF_BR[:int(n_GGF_BR*r_tr)]\n",
    "    idx_VBF_SR_vl = idx_VBF_SR[int(n_VBF_SR*r_tr):n_VBF_SR]\n",
    "    idx_VBF_BR_vl = idx_VBF_BR[int(n_VBF_BR*r_tr):n_VBF_BR]\n",
    "    idx_GGF_SR_vl = idx_GGF_SR[int(n_GGF_SR*r_tr):n_GGF_SR]\n",
    "    idx_GGF_BR_vl = idx_GGF_BR[int(n_GGF_BR*r_tr):n_GGF_BR]\n",
    "    idx_VBF_SR_te = idx_VBF_SR[n_VBF_SR:]\n",
    "    idx_VBF_BR_te = idx_VBF_BR[n_VBF_BR:]\n",
    "    idx_GGF_SR_te = idx_GGF_SR[n_GGF_SR:]\n",
    "    idx_GGF_BR_te = idx_GGF_BR[n_GGF_BR:]\n",
    "\n",
    "    print(f'Preparing dataset from {npy_dirs}')\n",
    "    for npy_dir in npy_dirs:\n",
    "\n",
    "        npy_dir = Path(npy_dir)\n",
    "        data_VBF_SR = np.load(npy_dir / 'VBF_in_SR-data.npy')\n",
    "        data_VBF_BR = np.load(npy_dir / 'VBF_in_BR-data.npy')\n",
    "        data_GGF_SR = np.load(npy_dir / 'GGF_in_SR-data.npy')\n",
    "        data_GGF_BR = np.load(npy_dir / 'GGF_in_BR-data.npy')\n",
    "\n",
    "        new_data_tr = np.concatenate([\n",
    "            data_VBF_SR[idx_VBF_SR_tr],\n",
    "            data_GGF_SR[idx_GGF_SR_tr],\n",
    "            data_VBF_BR[idx_VBF_BR_tr],\n",
    "            data_GGF_BR[idx_GGF_BR_tr]\n",
    "        ], axis=0)\n",
    "        new_data_vl = np.concatenate([\n",
    "            data_VBF_SR[idx_VBF_SR_vl],\n",
    "            data_GGF_SR[idx_GGF_SR_vl],\n",
    "            data_VBF_BR[idx_VBF_BR_vl],\n",
    "            data_GGF_BR[idx_GGF_BR_vl]\n",
    "        ], axis=0)\n",
    "        # new_data_te = np.concatenate([\n",
    "        #     data_VBF_SR[idx_VBF_SR_te],\n",
    "        #     data_VBF_BR[idx_VBF_BR_te],\n",
    "        #     data_GGF_SR[idx_GGF_SR_te],\n",
    "        #     data_GGF_BR[idx_GGF_BR_te],\n",
    "        # ], axis=0)\n",
    "\n",
    "        if data_tr is None:\n",
    "            data_tr = new_data_tr\n",
    "            data_vl = new_data_vl\n",
    "            # data_te = new_data_te\n",
    "        else:\n",
    "            data_tr = np.concatenate([data_tr, new_data_tr], axis=0)\n",
    "            data_vl = np.concatenate([data_vl, new_data_vl], axis=0)\n",
    "            # data_te = np.concatenate([data_te, new_data_te], axis=0)\n",
    "\n",
    "        new_label_tr = np.zeros(new_data_tr.shape[0])\n",
    "        new_label_tr[:idx_VBF_SR_tr.shape[0] + idx_GGF_SR_tr.shape[0]] = 1\n",
    "        new_label_vl = np.zeros(new_data_vl.shape[0])\n",
    "        new_label_vl[:idx_VBF_SR_vl.shape[0] + idx_GGF_SR_vl.shape[0]] = 1\n",
    "        # new_label_te = np.zeros(new_data_te.shape[0])\n",
    "        # new_label_te[:n_test] = 1\n",
    "\n",
    "        if label_tr is None:\n",
    "            label_tr = new_label_tr\n",
    "            label_vl = new_label_vl\n",
    "            # label_te = new_label_te\n",
    "        else:\n",
    "            label_tr = np.concatenate([label_tr, new_label_tr])\n",
    "            label_vl = np.concatenate([label_vl, new_label_vl])\n",
    "            # label_te = np.concatenate([label_te, new_label_te])\n",
    "\n",
    "    new_data_te = np.concatenate([\n",
    "        data_VBF_SR[idx_VBF_SR_te],\n",
    "        data_VBF_BR[idx_VBF_BR_te],\n",
    "        data_GGF_SR[idx_GGF_SR_te],\n",
    "        data_GGF_BR[idx_GGF_BR_te],\n",
    "    ], axis=0)\n",
    "    data_te = new_data_te\n",
    "\n",
    "    new_label_te = np.zeros(new_data_te.shape[0])\n",
    "    new_label_te[:n_test] = 1\n",
    "    label_te = new_label_te\n",
    "\n",
    "    return data_tr, data_vl, data_te, label_tr, label_vl, label_te\n",
    "\n",
    "def create_test_sample_from(npy_dirs: list, nevents: tuple, ratios=(0.8, 0.2), seed=0):\n",
    "    # npy_dirs: list of npy directories\n",
    "    # nevents: tuple of (n_VBF_SR, n_VBF_BR, n_GGF_SR, n_GGF_BR)\n",
    "    # ratios: tuple of (r_train, r_val)\n",
    "\n",
    "    npy_dir0 = Path(npy_dirs[0])\n",
    "\n",
    "    data_VBF_SR = np.load(npy_dir0 / 'VBF_in_SR-data.npy')\n",
    "    data_VBF_BR = np.load(npy_dir0 / 'VBF_in_BR-data.npy')\n",
    "    data_GGF_SR = np.load(npy_dir0 / 'GGF_in_SR-data.npy')\n",
    "    data_GGF_BR = np.load(npy_dir0 / 'GGF_in_BR-data.npy')\n",
    "\n",
    "    n_VBF_SR, n_GGF_SR, n_VBF_BR, n_GGF_BR = nevents\n",
    "    n_test = 10000\n",
    "    n_VBF_SR_test = int(data_VBF_SR.shape[0] / (data_VBF_SR.shape[0] + data_VBF_BR.shape[0]) * n_test)\n",
    "    n_VBF_BR_test = n_test - n_VBF_SR_test\n",
    "    n_GGF_SR_test = int(data_GGF_SR.shape[0] / (data_GGF_SR.shape[0] + data_GGF_BR.shape[0]) * n_test)\n",
    "    n_GGF_BR_test = n_test - n_GGF_SR_test\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    print(data_GGF_SR.shape[0], data_GGF_BR.shape[0], data_VBF_SR.shape[0], data_VBF_BR.shape[0])\n",
    "    idx_VBF_SR = np.random.choice(data_VBF_SR.shape[0], n_VBF_SR + n_VBF_SR_test, replace=False)\n",
    "    idx_VBF_BR = np.random.choice(data_VBF_BR.shape[0], n_VBF_BR + n_VBF_BR_test, replace=False)\n",
    "    idx_GGF_SR = np.random.choice(data_GGF_SR.shape[0], n_GGF_SR + n_GGF_SR_test, replace=False)\n",
    "    idx_GGF_BR = np.random.choice(data_GGF_BR.shape[0], n_GGF_BR + n_GGF_BR_test, replace=False)\n",
    "\n",
    "    idx_VBF_SR_te = idx_VBF_SR[n_VBF_SR:]\n",
    "    idx_VBF_BR_te = idx_VBF_BR[n_VBF_BR:]\n",
    "    idx_GGF_SR_te = idx_GGF_SR[n_GGF_SR:]\n",
    "    idx_GGF_BR_te = idx_GGF_BR[n_GGF_BR:]\n",
    "\n",
    "    print(f'Preparing dataset from {npy_dirs}')\n",
    "\n",
    "    new_data_te = np.concatenate([\n",
    "        data_VBF_SR[idx_VBF_SR_te],\n",
    "        data_VBF_BR[idx_VBF_BR_te],\n",
    "        data_GGF_SR[idx_GGF_SR_te],\n",
    "        data_GGF_BR[idx_GGF_BR_te],\n",
    "    ], axis=0)\n",
    "    data_te = new_data_te\n",
    "\n",
    "    new_label_te = np.zeros(new_data_te.shape[0])\n",
    "    new_label_te[:n_test] = 1\n",
    "    label_te = new_label_te\n",
    "\n",
    "    return data_te, label_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2626b1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nevent_in_SR_BR(GGF_cutflow_file='../Sample/selection_results_GGF_300_3.1.npy', VBF_cutflow_file='../Sample/selection_results_VBF_300_3.1.npy', L=300, cut_type='mjj'):\n",
    "    # https://twiki.cern.ch/twiki/bin/view/LHCPhysics/CERNYellowReportPageAt14TeV\n",
    "    cross_section_GGF = 54.67 * 1000\n",
    "    cross_section_VBF = 4.278 * 1000\n",
    "    # https://twiki.cern.ch/twiki/bin/view/LHCPhysics/CERNYellowReportPageBR\n",
    "    BR_Haa = 0.00227\n",
    "\n",
    "    GGF_selection = np.load(GGF_cutflow_file, allow_pickle=True).item()\n",
    "    VBF_selection = np.load(VBF_cutflow_file, allow_pickle=True).item()\n",
    "\n",
    "    if cut_type == 'mjj':\n",
    "        n_GGF_SR = cross_section_GGF * GGF_selection['cutflow_number']['mjj: sig region'] / GGF_selection['cutflow_number']['Total'] * BR_Haa * L\n",
    "        n_GGF_BR = cross_section_GGF * GGF_selection['cutflow_number']['mjj: bkg region'] / GGF_selection['cutflow_number']['Total'] * BR_Haa * L\n",
    "        n_VBF_SR = cross_section_VBF * VBF_selection['cutflow_number']['mjj: sig region'] / VBF_selection['cutflow_number']['Total'] * BR_Haa * L\n",
    "        n_VBF_BR = cross_section_VBF * VBF_selection['cutflow_number']['mjj: bkg region'] / VBF_selection['cutflow_number']['Total'] * BR_Haa * L\n",
    "    elif cut_type == 'deta':\n",
    "        n_GGF_SR = cross_section_GGF * GGF_selection['cutflow_number']['deta: sig region'] / GGF_selection['cutflow_number']['Total'] * BR_Haa * L\n",
    "        n_GGF_BR = cross_section_GGF * GGF_selection['cutflow_number']['deta: bkg region'] / GGF_selection['cutflow_number']['Total'] * BR_Haa * L\n",
    "        n_VBF_SR = cross_section_VBF * VBF_selection['cutflow_number']['deta: sig region'] / VBF_selection['cutflow_number']['Total'] * BR_Haa * L\n",
    "        n_VBF_BR = cross_section_VBF * VBF_selection['cutflow_number']['deta: bkg region'] / VBF_selection['cutflow_number']['Total'] * BR_Haa * L\n",
    "    elif cut_type == 'mjj, deta':\n",
    "        n_GGF_SR = cross_section_GGF * GGF_selection['cutflow_number']['mjj, deta: sig region'] / GGF_selection['cutflow_number']['Total'] * BR_Haa * L\n",
    "        n_GGF_BR = cross_section_GGF * GGF_selection['cutflow_number']['mjj, deta: bkg region'] / GGF_selection['cutflow_number']['Total'] * BR_Haa * L\n",
    "        n_VBF_SR = cross_section_VBF * VBF_selection['cutflow_number']['mjj, deta: sig region'] / VBF_selection['cutflow_number']['Total'] * BR_Haa * L\n",
    "        n_VBF_BR = cross_section_VBF * VBF_selection['cutflow_number']['mjj, deta: bkg region'] / VBF_selection['cutflow_number']['Total'] * BR_Haa * L\n",
    "    elif cut_type == 'gluon_jet_2':\n",
    "        n_GGF_SR = cross_section_GGF * GGF_selection['cutflow_number']['two gluon jet: sig region'] / GGF_selection['cutflow_number']['Total'] * BR_Haa * L\n",
    "        n_GGF_BR = cross_section_GGF * GGF_selection['cutflow_number']['two gluon jet: bkg region'] / GGF_selection['cutflow_number']['Total'] * BR_Haa * L\n",
    "        n_VBF_SR = cross_section_VBF * VBF_selection['cutflow_number']['two gluon jet: sig region'] / VBF_selection['cutflow_number']['Total'] * BR_Haa * L\n",
    "        n_VBF_BR = cross_section_VBF * VBF_selection['cutflow_number']['two gluon jet: bkg region'] / VBF_selection['cutflow_number']['Total'] * BR_Haa * L\n",
    "    elif cut_type == 'gluon_jet_1':\n",
    "        n_GGF_SR = cross_section_GGF * GGF_selection['cutflow_number']['one gluon jet: sig region'] / GGF_selection['cutflow_number']['Total'] * BR_Haa * L\n",
    "        n_GGF_BR = cross_section_GGF * GGF_selection['cutflow_number']['one gluon jet: bkg region'] / GGF_selection['cutflow_number']['Total'] * BR_Haa * L\n",
    "        n_VBF_SR = cross_section_VBF * VBF_selection['cutflow_number']['one gluon jet: sig region'] / VBF_selection['cutflow_number']['Total'] * BR_Haa * L\n",
    "        n_VBF_BR = cross_section_VBF * VBF_selection['cutflow_number']['one gluon jet: bkg region'] / VBF_selection['cutflow_number']['Total'] * BR_Haa * L\n",
    "    elif cut_type == 'quark_jet_2':\n",
    "        n_GGF_SR = cross_section_GGF * GGF_selection['cutflow_number']['two quark jet: sig region'] / GGF_selection['cutflow_number']['Total'] * BR_Haa * L\n",
    "        n_GGF_BR = cross_section_GGF * GGF_selection['cutflow_number']['two quark jet: bkg region'] / GGF_selection['cutflow_number']['Total'] * BR_Haa * L\n",
    "        n_VBF_SR = cross_section_VBF * VBF_selection['cutflow_number']['two quark jet: sig region'] / VBF_selection['cutflow_number']['Total'] * BR_Haa * L\n",
    "        n_VBF_BR = cross_section_VBF * VBF_selection['cutflow_number']['two quark jet: bkg region'] / VBF_selection['cutflow_number']['Total'] * BR_Haa * L\n",
    "    elif cut_type == 'quark_jet_1':\n",
    "        n_GGF_SR = cross_section_GGF * GGF_selection['cutflow_number']['one quark jet: sig region'] / GGF_selection['cutflow_number']['Total'] * BR_Haa * L\n",
    "        n_GGF_BR = cross_section_GGF * GGF_selection['cutflow_number']['one quark jet: bkg region'] / GGF_selection['cutflow_number']['Total'] * BR_Haa * L\n",
    "        n_VBF_SR = cross_section_VBF * VBF_selection['cutflow_number']['one quark jet: sig region'] / VBF_selection['cutflow_number']['Total'] * BR_Haa * L\n",
    "        n_VBF_BR = cross_section_VBF * VBF_selection['cutflow_number']['one quark jet: bkg region'] / VBF_selection['cutflow_number']['Total'] * BR_Haa * L\n",
    "    elif cut_type == 'quark_gluon_jet_2':\n",
    "        n_GGF_SR = cross_section_GGF * GGF_selection['cutflow_number']['two quark jet: sig region'] / GGF_selection['cutflow_number']['Total'] * BR_Haa * L\n",
    "        n_GGF_BR = cross_section_GGF * GGF_selection['cutflow_number']['two gluon jet: bkg region'] / GGF_selection['cutflow_number']['Total'] * BR_Haa * L\n",
    "        n_VBF_SR = cross_section_VBF * VBF_selection['cutflow_number']['two quark jet: sig region'] / VBF_selection['cutflow_number']['Total'] * BR_Haa * L\n",
    "        n_VBF_BR = cross_section_VBF * VBF_selection['cutflow_number']['two gluon jet: bkg region'] / VBF_selection['cutflow_number']['Total'] * BR_Haa * L\n",
    "    else:\n",
    "        raise ValueError('cut_type must be mjj, deta, or mjj, or deta, or gluon_jet')\n",
    "    return n_VBF_SR, n_GGF_SR, n_VBF_BR, n_GGF_BR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d198aa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_size(y):\n",
    "    if len(y.shape) == 1:\n",
    "        ns = (y == 1).sum()\n",
    "        nb = (y == 0).sum()\n",
    "    else:\n",
    "        ns = (y.argmax(axis=1) == 1).sum()\n",
    "        nb = (y.argmax(axis=1) == 0).sum()\n",
    "    print(ns, nb)\n",
    "    return ns, nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "016c0cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pt_normalization(X):\n",
    "    # input shape: (n, res, res, 3)\n",
    "    mean = np.mean(X, axis=(1, 2), keepdims=True)\n",
    "    std = np.std(X, axis=(1, 2), keepdims=True)\n",
    "    epsilon = 1e-8\n",
    "    std = np.where(std < epsilon, epsilon, std)\n",
    "    return (X - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b2a71a",
   "metadata": {},
   "source": [
    "# Training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4ce2566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_highest_accuracy(y_true, y_pred):\n",
    "    _, _, thresholds = roc_curve(y_true, y_pred)\n",
    "    # compute highest accuracy\n",
    "    thresholds = np.array(thresholds)\n",
    "    if len(thresholds) > 1000:\n",
    "        thresholds = np.percentile(thresholds, np.linspace(0, 100, 1001))\n",
    "    accuracy_scores = []\n",
    "    for threshold in thresholds:\n",
    "        accuracy_scores.append(accuracy_score(y_true, y_pred > threshold))\n",
    "\n",
    "    accuracies = np.array(accuracy_scores)\n",
    "    return accuracies.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e43829d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "with open('params.json', 'r') as f:\n",
    "    params = json.load(f)\n",
    "\n",
    "BATCH_SIZE = params['BATCH_SIZE']\n",
    "EPOCHS = params['EPOCHS']\n",
    "patience = params['patience']\n",
    "min_delta = params['min_delta']\n",
    "learning_rate = params['learning_rate']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08969573",
   "metadata": {},
   "source": [
    "## Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ae5f692",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m n_events \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mint\u001b[39m(n_SR_VBF), \u001b[38;5;28mint\u001b[39m(n_SR_GGF), \u001b[38;5;28mint\u001b[39m(n_BR_VBF), \u001b[38;5;28mint\u001b[39m(n_BR_GGF))\n\u001b[1;32m     26\u001b[0m npy_paths \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../Sample/data/quark_jet_2_cut/pre-processing/40x40/\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 27\u001b[0m X_test, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_test_sample_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnpy_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_events\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mr_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m X_test \u001b[38;5;241m=\u001b[39m pt_normalization(X_test)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Compute ACC & AUC\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 119\u001b[0m, in \u001b[0;36mcreate_test_sample_from\u001b[0;34m(npy_dirs, nevents, ratios, seed)\u001b[0m\n\u001b[1;32m    116\u001b[0m npy_dir0 \u001b[38;5;241m=\u001b[39m Path(npy_dirs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    118\u001b[0m data_VBF_SR \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(npy_dir0 \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVBF_in_SR-data.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 119\u001b[0m data_VBF_BR \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnpy_dir0\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mVBF_in_BR-data.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m data_GGF_SR \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(npy_dir0 \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGGF_in_SR-data.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    121\u001b[0m data_GGF_BR \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(npy_dir0 \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGGF_in_BR-data.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/tf2/lib/python3.8/site-packages/numpy/lib/npyio.py:432\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mopen_memmap(file, mode\u001b[38;5;241m=\u001b[39mmmap_mode,\n\u001b[1;32m    430\u001b[0m                                   max_header_size\u001b[38;5;241m=\u001b[39mmax_header_size)\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 432\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_header_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;66;03m# Try a pickle\u001b[39;00m\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n",
      "File \u001b[0;32m~/.conda/envs/tf2/lib/python3.8/site-packages/numpy/lib/format.py:801\u001b[0m, in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m isfileobj(fp):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;66;03m# We can use the fast fromfile() function.\u001b[39;00m\n\u001b[0;32m--> 801\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    803\u001b[0m         \u001b[38;5;66;03m# This is not a real file. We have to read it the\u001b[39;00m\n\u001b[1;32m    804\u001b[0m         \u001b[38;5;66;03m# memory-intensive way.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# not correctly instantiate zero-width string dtypes; see\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# https://github.com/numpy/numpy/pull/6430\u001b[39;00m\n\u001b[1;32m    814\u001b[0m         array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mndarray(count, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ACC_preprocess, AUC_preprocess = [], []\n",
    "ACC_wo_preprocess, AUC_wo_preprocess = [], []\n",
    "for i in range(1, 11):\n",
    "    # config_path = f'./config_files/quark_jet_2_cut_L_3000_pT_norm_phi_aug_15_config_{i:02}.json'\n",
    "    config_path = f'./config_files/quark_jet_2_cut_L_3000_pT_norm_config_{i:02}.json'\n",
    "    # Read config file\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    seed = config['seed']\n",
    "    luminosity = config['luminosity']\n",
    "    cut_type = config['cut_type']\n",
    "    model_name = config['model_name']\n",
    "\n",
    "    GGF_cutflow_file = config['GGF_cutflow_file']\n",
    "    VBF_cutflow_file = config['VBF_cutflow_file']\n",
    "\n",
    "    model_name = config['model_name']\n",
    "\n",
    "\n",
    "    # Sampling dataset\n",
    "    r_train, r_val = 0.8, 0.2\n",
    "    n_SR_VBF, n_SR_GGF, n_BR_VBF, n_BR_GGF = compute_nevent_in_SR_BR(GGF_cutflow_file, VBF_cutflow_file, luminosity, cut_type)\n",
    "    n_events = (int(n_SR_VBF), int(n_SR_GGF), int(n_BR_VBF), int(n_BR_GGF))\n",
    "\n",
    "    npy_paths = ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
    "    X_test, y_test = create_test_sample_from(npy_paths, n_events, (r_train, r_val), seed=seed)\n",
    "    X_test = pt_normalization(X_test)\n",
    "\n",
    "    # Compute ACC & AUC\n",
    "    save_model_name = f'./CNN_models/last_model_GGF_VBF_CWoLa_{model_name}/'\n",
    "    loaded_model = tf.keras.models.load_model(save_model_name)\n",
    "    y_pred = loaded_model.predict(X_test, batch_size=BATCH_SIZE)\n",
    "\n",
    "    ACC_preprocess.append(get_highest_accuracy(y_test, y_pred))\n",
    "    AUC_preprocess.append(roc_auc_score(y_test, y_pred))\n",
    "\n",
    "    npy_paths = ['../Sample/data/quark_jet_2_cut/40x40/']\n",
    "    X_test, y_test = create_test_sample_from(npy_paths, n_events, (r_train, r_val), seed=seed)\n",
    "    X_test = pt_normalization(X_test)\n",
    "\n",
    "    # Compute ACC & AUC\n",
    "    save_model_name = f'./CNN_models/last_model_GGF_VBF_CWoLa_{model_name}/'\n",
    "    loaded_model = tf.keras.models.load_model(save_model_name)\n",
    "    y_pred = loaded_model.predict(X_test, batch_size=BATCH_SIZE)\n",
    "\n",
    "    ACC_wo_preprocess.append(get_highest_accuracy(y_test, y_pred))\n",
    "    AUC_wo_preprocess.append(roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86ae57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-process ACC and AUC\n",
      "ACC: 0.637 ± 0.007\n",
      "AUC: 0.686 ± 0.008\n",
      "Without pre-process ACC and AUC\n",
      "ACC: 0.625 ± 0.006\n",
      "AUC: 0.669 ± 0.008\n",
      "$0.637 \\pm 0.007$ & $0.686 \\pm 0.008$ & $0.625 \\pm 0.006$ & $0.669 \\pm 0.008$\n"
     ]
    }
   ],
   "source": [
    "ACC = np.array(ACC_preprocess)\n",
    "AUC = np.array(AUC_preprocess)\n",
    "\n",
    "latex_txt = ''\n",
    "\n",
    "print('Pre-process ACC and AUC')\n",
    "print(f'ACC: {ACC.mean():.3f} ± {ACC.std():.3f}')\n",
    "print(f'AUC: {AUC.mean():.3f} ± {AUC.std():.3f}')\n",
    "latex_txt += f'${ACC.mean():.3f} \\pm {ACC.std():.3f}$ & ${AUC.mean():.3f} \\pm {AUC.std():.3f}$'\n",
    "\n",
    "ACC = np.array(ACC_wo_preprocess)\n",
    "AUC = np.array(AUC_wo_preprocess)\n",
    "\n",
    "print('Without pre-process ACC and AUC')\n",
    "print(f'ACC: {ACC.mean():.3f} ± {ACC.std():.3f}')\n",
    "print(f'AUC: {AUC.mean():.3f} ± {AUC.std():.3f}')\n",
    "latex_txt += f' & ${ACC.mean():.3f} \\pm {ACC.std():.3f}$ & ${AUC.mean():.3f} \\pm {AUC.std():.3f}$'\n",
    "print(latex_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34339b59",
   "metadata": {},
   "source": [
    "## $\\phi$-shifting: +5, +10, +15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f3ffaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 9ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "ACC_preprocess, AUC_preprocess = [], []\n",
    "ACC_wo_preprocess, AUC_wo_preprocess = [], []\n",
    "for i in range(1, 11):\n",
    "    config_path = f'./config_files/quark_jet_2_cut_L_3000_pT_norm_phi_aug_5_config_{i:02}.json'\n",
    "    # Read config file\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    seed = config['seed']\n",
    "    luminosity = config['luminosity']\n",
    "    cut_type = config['cut_type']\n",
    "    model_name = config['model_name']\n",
    "\n",
    "    GGF_cutflow_file = config['GGF_cutflow_file']\n",
    "    VBF_cutflow_file = config['VBF_cutflow_file']\n",
    "\n",
    "    model_name = config['model_name']\n",
    "\n",
    "\n",
    "    # Sampling dataset\n",
    "    r_train, r_val = 0.8, 0.2\n",
    "    n_SR_VBF, n_SR_GGF, n_BR_VBF, n_BR_GGF = compute_nevent_in_SR_BR(GGF_cutflow_file, VBF_cutflow_file, luminosity, cut_type)\n",
    "    n_events = (int(n_SR_VBF), int(n_SR_GGF), int(n_BR_VBF), int(n_BR_GGF))\n",
    "\n",
    "    npy_paths = ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
    "    X_test, y_test = create_test_sample_from(npy_paths, n_events, (r_train, r_val), seed=seed)\n",
    "    X_test = pt_normalization(X_test)\n",
    "\n",
    "    # Compute ACC & AUC\n",
    "    save_model_name = f'./CNN_models/last_model_GGF_VBF_CWoLa_{model_name}/'\n",
    "    loaded_model = tf.keras.models.load_model(save_model_name)\n",
    "    y_pred = loaded_model.predict(X_test, batch_size=BATCH_SIZE)\n",
    "\n",
    "    ACC_preprocess.append(get_highest_accuracy(y_test, y_pred))\n",
    "    AUC_preprocess.append(roc_auc_score(y_test, y_pred))\n",
    "\n",
    "    npy_paths = ['../Sample/data/quark_jet_2_cut/40x40/']\n",
    "    X_test, y_test = create_test_sample_from(npy_paths, n_events, (r_train, r_val), seed=seed)\n",
    "    X_test = pt_normalization(X_test)\n",
    "\n",
    "    # Compute ACC & AUC\n",
    "    save_model_name = f'./CNN_models/last_model_GGF_VBF_CWoLa_{model_name}/'\n",
    "    loaded_model = tf.keras.models.load_model(save_model_name)\n",
    "    y_pred = loaded_model.predict(X_test, batch_size=BATCH_SIZE)\n",
    "\n",
    "    ACC_wo_preprocess.append(get_highest_accuracy(y_test, y_pred))\n",
    "    AUC_wo_preprocess.append(roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c604ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-process ACC and AUC\n",
      "ACC: 0.682 ± 0.011\n",
      "AUC: 0.735 ± 0.013\n",
      "Without pre-process ACC and AUC\n",
      "ACC: 0.669 ± 0.011\n",
      "AUC: 0.720 ± 0.015\n",
      " & $0.682 \\pm 0.011$ & $0.735 \\pm 0.013$ & $0.669 \\pm 0.011$ & $0.720 \\pm 0.015$\n"
     ]
    }
   ],
   "source": [
    "ACC = np.array(ACC_preprocess)\n",
    "AUC = np.array(AUC_preprocess)\n",
    "\n",
    "latex_txt = ''\n",
    "\n",
    "print('Pre-process ACC and AUC')\n",
    "print(f'ACC: {ACC.mean():.3f} ± {ACC.std():.3f}')\n",
    "print(f'AUC: {AUC.mean():.3f} ± {AUC.std():.3f}')\n",
    "latex_txt += f' & ${ACC.mean():.3f} \\pm {ACC.std():.3f}$ & ${AUC.mean():.3f} \\pm {AUC.std():.3f}$'\n",
    "\n",
    "ACC = np.array(ACC_wo_preprocess)\n",
    "AUC = np.array(AUC_wo_preprocess)\n",
    "\n",
    "print('Without pre-process ACC and AUC')\n",
    "print(f'ACC: {ACC.mean():.3f} ± {ACC.std():.3f}')\n",
    "print(f'AUC: {AUC.mean():.3f} ± {AUC.std():.3f}')\n",
    "latex_txt += f' & ${ACC.mean():.3f} \\pm {ACC.std():.3f}$ & ${AUC.mean():.3f} \\pm {AUC.std():.3f}$'\n",
    "print(latex_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177a953f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "ACC_preprocess, AUC_preprocess = [], []\n",
    "ACC_wo_preprocess, AUC_wo_preprocess = [], []\n",
    "for i in range(1, 11):\n",
    "    config_path = f'./config_files/quark_jet_2_cut_L_3000_pT_norm_phi_aug_10_config_{i:02}.json'\n",
    "    # Read config file\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    seed = config['seed']\n",
    "    luminosity = config['luminosity']\n",
    "    cut_type = config['cut_type']\n",
    "    model_name = config['model_name']\n",
    "\n",
    "    GGF_cutflow_file = config['GGF_cutflow_file']\n",
    "    VBF_cutflow_file = config['VBF_cutflow_file']\n",
    "\n",
    "    model_name = config['model_name']\n",
    "\n",
    "\n",
    "    # Sampling dataset\n",
    "    r_train, r_val = 0.8, 0.2\n",
    "    n_SR_VBF, n_SR_GGF, n_BR_VBF, n_BR_GGF = compute_nevent_in_SR_BR(GGF_cutflow_file, VBF_cutflow_file, luminosity, cut_type)\n",
    "    n_events = (int(n_SR_VBF), int(n_SR_GGF), int(n_BR_VBF), int(n_BR_GGF))\n",
    "\n",
    "    npy_paths = ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
    "    X_test, y_test = create_test_sample_from(npy_paths, n_events, (r_train, r_val), seed=seed)\n",
    "    X_test = pt_normalization(X_test)\n",
    "\n",
    "    # Compute ACC & AUC\n",
    "    save_model_name = f'./CNN_models/last_model_GGF_VBF_CWoLa_{model_name}/'\n",
    "    loaded_model = tf.keras.models.load_model(save_model_name)\n",
    "    y_pred = loaded_model.predict(X_test, batch_size=BATCH_SIZE)\n",
    "\n",
    "    ACC_preprocess.append(get_highest_accuracy(y_test, y_pred))\n",
    "    AUC_preprocess.append(roc_auc_score(y_test, y_pred))\n",
    "\n",
    "    npy_paths = ['../Sample/data/quark_jet_2_cut/40x40/']\n",
    "    X_test, y_test = create_test_sample_from(npy_paths, n_events, (r_train, r_val), seed=seed)\n",
    "    X_test = pt_normalization(X_test)\n",
    "\n",
    "    # Compute ACC & AUC\n",
    "    save_model_name = f'./CNN_models/last_model_GGF_VBF_CWoLa_{model_name}/'\n",
    "    loaded_model = tf.keras.models.load_model(save_model_name)\n",
    "    y_pred = loaded_model.predict(X_test, batch_size=BATCH_SIZE)\n",
    "\n",
    "    ACC_wo_preprocess.append(get_highest_accuracy(y_test, y_pred))\n",
    "    AUC_wo_preprocess.append(roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f2d8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-process ACC and AUC\n",
      "ACC: 0.685 ± 0.008\n",
      "AUC: 0.739 ± 0.010\n",
      "Without pre-process ACC and AUC\n",
      "ACC: 0.673 ± 0.008\n",
      "AUC: 0.726 ± 0.010\n",
      " & $0.685 \\pm 0.008$ & $0.739 \\pm 0.010$ & $0.673 \\pm 0.008$ & $0.726 \\pm 0.010$\n"
     ]
    }
   ],
   "source": [
    "ACC = np.array(ACC_preprocess)\n",
    "AUC = np.array(AUC_preprocess)\n",
    "\n",
    "latex_txt = ''\n",
    "\n",
    "print('Pre-process ACC and AUC')\n",
    "print(f'ACC: {ACC.mean():.3f} ± {ACC.std():.3f}')\n",
    "print(f'AUC: {AUC.mean():.3f} ± {AUC.std():.3f}')\n",
    "latex_txt += f' & ${ACC.mean():.3f} \\pm {ACC.std():.3f}$ & ${AUC.mean():.3f} \\pm {AUC.std():.3f}$'\n",
    "\n",
    "ACC = np.array(ACC_wo_preprocess)\n",
    "AUC = np.array(AUC_wo_preprocess)\n",
    "\n",
    "print('Without pre-process ACC and AUC')\n",
    "print(f'ACC: {ACC.mean():.3f} ± {ACC.std():.3f}')\n",
    "print(f'AUC: {AUC.mean():.3f} ± {AUC.std():.3f}')\n",
    "latex_txt += f' & ${ACC.mean():.3f} \\pm {ACC.std():.3f}$ & ${AUC.mean():.3f} \\pm {AUC.std():.3f}$'\n",
    "print(latex_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01baacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 8ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "ACC_preprocess, AUC_preprocess = [], []\n",
    "ACC_wo_preprocess, AUC_wo_preprocess = [], []\n",
    "for i in range(1, 11):\n",
    "    config_path = f'./config_files/quark_jet_2_cut_L_3000_pT_norm_phi_aug_15_config_{i:02}.json'\n",
    "    # Read config file\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    seed = config['seed']\n",
    "    luminosity = config['luminosity']\n",
    "    cut_type = config['cut_type']\n",
    "    model_name = config['model_name']\n",
    "\n",
    "    GGF_cutflow_file = config['GGF_cutflow_file']\n",
    "    VBF_cutflow_file = config['VBF_cutflow_file']\n",
    "\n",
    "    model_name = config['model_name']\n",
    "\n",
    "\n",
    "    # Sampling dataset\n",
    "    r_train, r_val = 0.8, 0.2\n",
    "    n_SR_VBF, n_SR_GGF, n_BR_VBF, n_BR_GGF = compute_nevent_in_SR_BR(GGF_cutflow_file, VBF_cutflow_file, luminosity, cut_type)\n",
    "    n_events = (int(n_SR_VBF), int(n_SR_GGF), int(n_BR_VBF), int(n_BR_GGF))\n",
    "\n",
    "    npy_paths = ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
    "    X_test, y_test = create_test_sample_from(npy_paths, n_events, (r_train, r_val), seed=seed)\n",
    "    X_test = pt_normalization(X_test)\n",
    "\n",
    "    # Compute ACC & AUC\n",
    "    save_model_name = f'./CNN_models/last_model_GGF_VBF_CWoLa_{model_name}/'\n",
    "    loaded_model = tf.keras.models.load_model(save_model_name)\n",
    "    y_pred = loaded_model.predict(X_test, batch_size=BATCH_SIZE)\n",
    "\n",
    "    ACC_preprocess.append(get_highest_accuracy(y_test, y_pred))\n",
    "    AUC_preprocess.append(roc_auc_score(y_test, y_pred))\n",
    "\n",
    "    npy_paths = ['../Sample/data/quark_jet_2_cut/40x40/']\n",
    "    X_test, y_test = create_test_sample_from(npy_paths, n_events, (r_train, r_val), seed=seed)\n",
    "    X_test = pt_normalization(X_test)\n",
    "\n",
    "    # Compute ACC & AUC\n",
    "    save_model_name = f'./CNN_models/last_model_GGF_VBF_CWoLa_{model_name}/'\n",
    "    loaded_model = tf.keras.models.load_model(save_model_name)\n",
    "    y_pred = loaded_model.predict(X_test, batch_size=BATCH_SIZE)\n",
    "\n",
    "    ACC_wo_preprocess.append(get_highest_accuracy(y_test, y_pred))\n",
    "    AUC_wo_preprocess.append(roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6a94c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-process ACC and AUC\n",
      "ACC: 0.688 ± 0.007\n",
      "AUC: 0.743 ± 0.009\n",
      "Without pre-process ACC and AUC\n",
      "ACC: 0.674 ± 0.008\n",
      "AUC: 0.726 ± 0.008\n",
      " & $0.688 \\pm 0.007$ & $0.743 \\pm 0.009$ & $0.674 \\pm 0.008$ & $0.726 \\pm 0.008$\n"
     ]
    }
   ],
   "source": [
    "ACC = np.array(ACC_preprocess)\n",
    "AUC = np.array(AUC_preprocess)\n",
    "\n",
    "latex_txt = ''\n",
    "\n",
    "print('Pre-process ACC and AUC')\n",
    "print(f'ACC: {ACC.mean():.3f} ± {ACC.std():.3f}')\n",
    "print(f'AUC: {AUC.mean():.3f} ± {AUC.std():.3f}')\n",
    "latex_txt += f' & ${ACC.mean():.3f} \\pm {ACC.std():.3f}$ & ${AUC.mean():.3f} \\pm {AUC.std():.3f}$'\n",
    "\n",
    "ACC = np.array(ACC_wo_preprocess)\n",
    "AUC = np.array(AUC_wo_preprocess)\n",
    "\n",
    "print('Without pre-process ACC and AUC')\n",
    "print(f'ACC: {ACC.mean():.3f} ± {ACC.std():.3f}')\n",
    "print(f'AUC: {AUC.mean():.3f} ± {AUC.std():.3f}')\n",
    "latex_txt += f' & ${ACC.mean():.3f} \\pm {ACC.std():.3f}$ & ${AUC.mean():.3f} \\pm {AUC.std():.3f}$'\n",
    "print(latex_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f57011",
   "metadata": {},
   "source": [
    "## Only $\\phi$-shifting: +5, +10, +15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78f26612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:52:26.674635: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-23 15:52:28.731861: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22288 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:d8:00.0, compute capability: 8.6\n",
      "2025-05-23 15:52:34.952302: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/40 [========>.....................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:52:40.680402: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 10s 12ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 8ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 8ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 8ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 8ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 8ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 8ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 8ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 8ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 8ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 8ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 8ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 8ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 8ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 8ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 8ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 8ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 8ms/step\n"
     ]
    }
   ],
   "source": [
    "ACC_preprocess, AUC_preprocess = [], []\n",
    "ACC_wo_preprocess, AUC_wo_preprocess = [], []\n",
    "for i in range(1, 11):\n",
    "    config_path = f'./config_files/quark_jet_2_cut_L_3000_pT_norm_only_phi_aug_5_config_{i:02}.json'\n",
    "    # Read config file\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    seed = config['seed']\n",
    "    luminosity = config['luminosity']\n",
    "    cut_type = config['cut_type']\n",
    "    model_name = config['model_name']\n",
    "\n",
    "    GGF_cutflow_file = config['GGF_cutflow_file']\n",
    "    VBF_cutflow_file = config['VBF_cutflow_file']\n",
    "\n",
    "    model_name = config['model_name']\n",
    "\n",
    "\n",
    "    # Sampling dataset\n",
    "    r_train, r_val = 0.8, 0.2\n",
    "    n_SR_VBF, n_SR_GGF, n_BR_VBF, n_BR_GGF = compute_nevent_in_SR_BR(GGF_cutflow_file, VBF_cutflow_file, luminosity, cut_type)\n",
    "    n_events = (int(n_SR_VBF), int(n_SR_GGF), int(n_BR_VBF), int(n_BR_GGF))\n",
    "\n",
    "    npy_paths = ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
    "    X_test, y_test = create_test_sample_from(npy_paths, n_events, (r_train, r_val), seed=seed)\n",
    "    X_test = pt_normalization(X_test)\n",
    "\n",
    "    # Compute ACC & AUC\n",
    "    save_model_name = f'./CNN_models/last_model_GGF_VBF_CWoLa_{model_name}/'\n",
    "    loaded_model = tf.keras.models.load_model(save_model_name)\n",
    "    y_pred = loaded_model.predict(X_test, batch_size=BATCH_SIZE)\n",
    "\n",
    "    ACC_preprocess.append(get_highest_accuracy(y_test, y_pred))\n",
    "    AUC_preprocess.append(roc_auc_score(y_test, y_pred))\n",
    "\n",
    "    npy_paths = ['../Sample/data/quark_jet_2_cut/40x40/']\n",
    "    X_test, y_test = create_test_sample_from(npy_paths, n_events, (r_train, r_val), seed=seed)\n",
    "    X_test = pt_normalization(X_test)\n",
    "\n",
    "    # Compute ACC & AUC\n",
    "    save_model_name = f'./CNN_models/last_model_GGF_VBF_CWoLa_{model_name}/'\n",
    "    loaded_model = tf.keras.models.load_model(save_model_name)\n",
    "    y_pred = loaded_model.predict(X_test, batch_size=BATCH_SIZE)\n",
    "\n",
    "    ACC_wo_preprocess.append(get_highest_accuracy(y_test, y_pred))\n",
    "    AUC_wo_preprocess.append(roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cd4e3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-process ACC and AUC\n",
      "ACC: 0.682 ± 0.007\n",
      "AUC: 0.736 ± 0.009\n",
      "Without pre-process ACC and AUC\n",
      "ACC: 0.668 ± 0.007\n",
      "AUC: 0.718 ± 0.010\n",
      " & $0.682 \\pm 0.007$ & $0.736 \\pm 0.009$ & $0.668 \\pm 0.007$ & $0.718 \\pm 0.010$\n"
     ]
    }
   ],
   "source": [
    "ACC = np.array(ACC_preprocess)\n",
    "AUC = np.array(AUC_preprocess)\n",
    "\n",
    "latex_txt = ''\n",
    "\n",
    "print('Pre-process ACC and AUC')\n",
    "print(f'ACC: {ACC.mean():.3f} ± {ACC.std():.3f}')\n",
    "print(f'AUC: {AUC.mean():.3f} ± {AUC.std():.3f}')\n",
    "latex_txt += f' & ${ACC.mean():.3f} \\pm {ACC.std():.3f}$ & ${AUC.mean():.3f} \\pm {AUC.std():.3f}$'\n",
    "\n",
    "ACC = np.array(ACC_wo_preprocess)\n",
    "AUC = np.array(AUC_wo_preprocess)\n",
    "\n",
    "print('Without pre-process ACC and AUC')\n",
    "print(f'ACC: {ACC.mean():.3f} ± {ACC.std():.3f}')\n",
    "print(f'AUC: {AUC.mean():.3f} ± {AUC.std():.3f}')\n",
    "latex_txt += f' & ${ACC.mean():.3f} \\pm {ACC.std():.3f}$ & ${AUC.mean():.3f} \\pm {AUC.std():.3f}$'\n",
    "print(latex_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f50b3b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 8ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 8ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 8ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 8ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 8ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 8ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 8ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 8ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 8ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 8ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 8ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 8ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 8ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 8ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 8ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 8ms/step\n"
     ]
    }
   ],
   "source": [
    "ACC_preprocess, AUC_preprocess = [], []\n",
    "ACC_wo_preprocess, AUC_wo_preprocess = [], []\n",
    "for i in range(1, 11):\n",
    "    config_path = f'./config_files/quark_jet_2_cut_L_3000_pT_norm_only_phi_aug_10_config_{i:02}.json'\n",
    "    # Read config file\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    seed = config['seed']\n",
    "    luminosity = config['luminosity']\n",
    "    cut_type = config['cut_type']\n",
    "    model_name = config['model_name']\n",
    "\n",
    "    GGF_cutflow_file = config['GGF_cutflow_file']\n",
    "    VBF_cutflow_file = config['VBF_cutflow_file']\n",
    "\n",
    "    model_name = config['model_name']\n",
    "\n",
    "\n",
    "    # Sampling dataset\n",
    "    r_train, r_val = 0.8, 0.2\n",
    "    n_SR_VBF, n_SR_GGF, n_BR_VBF, n_BR_GGF = compute_nevent_in_SR_BR(GGF_cutflow_file, VBF_cutflow_file, luminosity, cut_type)\n",
    "    n_events = (int(n_SR_VBF), int(n_SR_GGF), int(n_BR_VBF), int(n_BR_GGF))\n",
    "\n",
    "    npy_paths = ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
    "    X_test, y_test = create_test_sample_from(npy_paths, n_events, (r_train, r_val), seed=seed)\n",
    "    X_test = pt_normalization(X_test)\n",
    "\n",
    "    # Compute ACC & AUC\n",
    "    save_model_name = f'./CNN_models/last_model_GGF_VBF_CWoLa_{model_name}/'\n",
    "    loaded_model = tf.keras.models.load_model(save_model_name)\n",
    "    y_pred = loaded_model.predict(X_test, batch_size=BATCH_SIZE)\n",
    "\n",
    "    ACC_preprocess.append(get_highest_accuracy(y_test, y_pred))\n",
    "    AUC_preprocess.append(roc_auc_score(y_test, y_pred))\n",
    "\n",
    "    npy_paths = ['../Sample/data/quark_jet_2_cut/40x40/']\n",
    "    X_test, y_test = create_test_sample_from(npy_paths, n_events, (r_train, r_val), seed=seed)\n",
    "    X_test = pt_normalization(X_test)\n",
    "\n",
    "    # Compute ACC & AUC\n",
    "    save_model_name = f'./CNN_models/last_model_GGF_VBF_CWoLa_{model_name}/'\n",
    "    loaded_model = tf.keras.models.load_model(save_model_name)\n",
    "    y_pred = loaded_model.predict(X_test, batch_size=BATCH_SIZE)\n",
    "\n",
    "    ACC_wo_preprocess.append(get_highest_accuracy(y_test, y_pred))\n",
    "    AUC_wo_preprocess.append(roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cbe7d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-process ACC and AUC\n",
      "ACC: 0.687 ± 0.010\n",
      "AUC: 0.740 ± 0.012\n",
      "Without pre-process ACC and AUC\n",
      "ACC: 0.675 ± 0.009\n",
      "AUC: 0.726 ± 0.011\n",
      " & $0.687 \\pm 0.010$ & $0.740 \\pm 0.012$ & $0.675 \\pm 0.009$ & $0.726 \\pm 0.011$\n"
     ]
    }
   ],
   "source": [
    "ACC = np.array(ACC_preprocess)\n",
    "AUC = np.array(AUC_preprocess)\n",
    "\n",
    "latex_txt = ''\n",
    "\n",
    "print('Pre-process ACC and AUC')\n",
    "print(f'ACC: {ACC.mean():.3f} ± {ACC.std():.3f}')\n",
    "print(f'AUC: {AUC.mean():.3f} ± {AUC.std():.3f}')\n",
    "latex_txt += f' & ${ACC.mean():.3f} \\pm {ACC.std():.3f}$ & ${AUC.mean():.3f} \\pm {AUC.std():.3f}$'\n",
    "\n",
    "ACC = np.array(ACC_wo_preprocess)\n",
    "AUC = np.array(AUC_wo_preprocess)\n",
    "\n",
    "print('Without pre-process ACC and AUC')\n",
    "print(f'ACC: {ACC.mean():.3f} ± {ACC.std():.3f}')\n",
    "print(f'AUC: {AUC.mean():.3f} ± {AUC.std():.3f}')\n",
    "latex_txt += f' & ${ACC.mean():.3f} \\pm {ACC.std():.3f}$ & ${AUC.mean():.3f} \\pm {AUC.std():.3f}$'\n",
    "print(latex_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e09181f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 8ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 7ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 8ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 8ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 8ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 8ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 8ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 8ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 8ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 8ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 8ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 8ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 8ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 8ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 8ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 8ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 8ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
      "40/40 [==============================] - 0s 8ms/step\n",
      "78945 83384 286751 45436\n",
      "Preparing dataset from ['../Sample/data/quark_jet_2_cut/40x40/']\n",
      "40/40 [==============================] - 0s 8ms/step\n"
     ]
    }
   ],
   "source": [
    "ACC_preprocess, AUC_preprocess = [], []\n",
    "ACC_wo_preprocess, AUC_wo_preprocess = [], []\n",
    "for i in range(1, 11):\n",
    "    config_path = f'./config_files/quark_jet_2_cut_L_3000_pT_norm_only_phi_aug_15_config_{i:02}.json'\n",
    "    # Read config file\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    seed = config['seed']\n",
    "    luminosity = config['luminosity']\n",
    "    cut_type = config['cut_type']\n",
    "    model_name = config['model_name']\n",
    "\n",
    "    GGF_cutflow_file = config['GGF_cutflow_file']\n",
    "    VBF_cutflow_file = config['VBF_cutflow_file']\n",
    "\n",
    "    model_name = config['model_name']\n",
    "\n",
    "\n",
    "    # Sampling dataset\n",
    "    r_train, r_val = 0.8, 0.2\n",
    "    n_SR_VBF, n_SR_GGF, n_BR_VBF, n_BR_GGF = compute_nevent_in_SR_BR(GGF_cutflow_file, VBF_cutflow_file, luminosity, cut_type)\n",
    "    n_events = (int(n_SR_VBF), int(n_SR_GGF), int(n_BR_VBF), int(n_BR_GGF))\n",
    "\n",
    "    npy_paths = ['../Sample/data/quark_jet_2_cut/pre-processing/40x40/']\n",
    "    X_test, y_test = create_test_sample_from(npy_paths, n_events, (r_train, r_val), seed=seed)\n",
    "    X_test = pt_normalization(X_test)\n",
    "\n",
    "    # Compute ACC & AUC\n",
    "    save_model_name = f'./CNN_models/last_model_GGF_VBF_CWoLa_{model_name}/'\n",
    "    loaded_model = tf.keras.models.load_model(save_model_name)\n",
    "    y_pred = loaded_model.predict(X_test, batch_size=BATCH_SIZE)\n",
    "\n",
    "    ACC_preprocess.append(get_highest_accuracy(y_test, y_pred))\n",
    "    AUC_preprocess.append(roc_auc_score(y_test, y_pred))\n",
    "\n",
    "    npy_paths = ['../Sample/data/quark_jet_2_cut/40x40/']\n",
    "    X_test, y_test = create_test_sample_from(npy_paths, n_events, (r_train, r_val), seed=seed)\n",
    "    X_test = pt_normalization(X_test)\n",
    "\n",
    "    # Compute ACC & AUC\n",
    "    save_model_name = f'./CNN_models/last_model_GGF_VBF_CWoLa_{model_name}/'\n",
    "    loaded_model = tf.keras.models.load_model(save_model_name)\n",
    "    y_pred = loaded_model.predict(X_test, batch_size=BATCH_SIZE)\n",
    "\n",
    "    ACC_wo_preprocess.append(get_highest_accuracy(y_test, y_pred))\n",
    "    AUC_wo_preprocess.append(roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20556959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-process ACC and AUC\n",
      "ACC: 0.687 ± 0.007\n",
      "AUC: 0.741 ± 0.010\n",
      "Without pre-process ACC and AUC\n",
      "ACC: 0.672 ± 0.009\n",
      "AUC: 0.725 ± 0.012\n",
      " & $0.687 \\pm 0.007$ & $0.741 \\pm 0.010$ & $0.672 \\pm 0.009$ & $0.725 \\pm 0.012$\n"
     ]
    }
   ],
   "source": [
    "ACC = np.array(ACC_preprocess)\n",
    "AUC = np.array(AUC_preprocess)\n",
    "\n",
    "latex_txt = ''\n",
    "\n",
    "print('Pre-process ACC and AUC')\n",
    "print(f'ACC: {ACC.mean():.3f} ± {ACC.std():.3f}')\n",
    "print(f'AUC: {AUC.mean():.3f} ± {AUC.std():.3f}')\n",
    "latex_txt += f' & ${ACC.mean():.3f} \\pm {ACC.std():.3f}$ & ${AUC.mean():.3f} \\pm {AUC.std():.3f}$'\n",
    "\n",
    "ACC = np.array(ACC_wo_preprocess)\n",
    "AUC = np.array(AUC_wo_preprocess)\n",
    "\n",
    "print('Without pre-process ACC and AUC')\n",
    "print(f'ACC: {ACC.mean():.3f} ± {ACC.std():.3f}')\n",
    "print(f'AUC: {AUC.mean():.3f} ± {AUC.std():.3f}')\n",
    "latex_txt += f' & ${ACC.mean():.3f} \\pm {ACC.std():.3f}$ & ${AUC.mean():.3f} \\pm {AUC.std():.3f}$'\n",
    "print(latex_txt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
