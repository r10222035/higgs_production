%!TEX program = xelatex
%!TEX options=--shell-escape
\documentclass[12pt]{article}

%
\usepackage[scheme=plain]{ctex}
%
\usepackage{fontspec}
%
\usepackage[margin = 1in]{geometry}

%
\usepackage[dvipsnames]{xcolor}
\usepackage[many]{tcolorbox}

%
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
%
\usepackage{tensor}
%
\usepackage{slashed}
\usepackage{physics}
\usepackage{simpler-wick}

%
\usepackage{mathtools}

%
\usepackage{bm}
\newcommand{\dbar}{\dif\hspace*{-0.18em}\bar{}\hspace*{0.2em}}
\DeclareMathAlphabet\mathbfcal{OMS}{cmsy}{b}{n}
%\usepackage{bbold}
\newcommand*{\dif}{\mathop{}\!\mathrm{d}}
\newcommand*{\euler}{\mathrm{e}}
\newcommand*{\imagi}{\mathrm{i}}

\renewcommand{\vec}[1]{\boldsymbol{\mathbf{#1}}}

\usepackage{caption}
\usepackage{multirow}
\usepackage{enumitem}

%
\usepackage{mathrsfs}
\usepackage{dsfont}

%
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=violet,
    filecolor=blue,
    urlcolor=blue,
    citecolor=cyan,
}

%
\usepackage{graphicx}
\usepackage{subfig}
%
\graphicspath{{../figures/}}

\usepackage{tikz}
\usetikzlibrary{positioning}
\usetikzlibrary{calc}

\usepackage{listings}
\usepackage{lstautogobble}
\lstset{
    basicstyle=\ttfamily,
    columns=fullflexible,
    autogobble=true,
}

%
\usepackage{indentfirst}
%
\setlength{\parindent}{2em}
\linespread{1.25}

%
% \setmainfont{Times New Roman}

\title{Note}
\author{Feng-Yang Hsieh}
\date{}

\begin{document}
\maketitle

\section{Higgs Production}% (fold)
\label{sec:higgs_production}
    We want to apply deep learning methods to distinguish vector boson fusion (VBF) from gluon-gluon fusion (GGF) and Higgs production at the LHC.

    We want to apply the CWoLa method, then we can use the real data without knowing the true label.
% section higgs_production (end)
\section{Sample Preparation}% (fold)
\label{sec:sample_preparation}
    \subsection{Monte Carlo samples}% (fold)
    \label{sub:monte_carlo_samples}
        We consider Standard Model (SM) di-photon Higgs events produced via GGF and VBF channels at a center-of-mass energy of $\sqrt{s} = 14$ TeV. The Higgs boson events are generated using \verb|MadGraph 3.3.1|~\cite{Alwall:2014hca} for both GGF and VBF production. The Higgs decays into the di-photon final state, and the parton showering and hadronization are simulated using \verb|Pythia 8.306|~\cite{Sjostrand:2014zea}. The detector simulation is conducted by \verb|Delphes 3.4.2|~\cite{deFavereau:2013fsa}. Jet reconstruction is performed using \verb|FastJet 3.3.2|~\cite{Cacciari:2011ma} with the anti-$k_t$ algorithm~\cite{Cacciari:2008gp} and a jet radius of $R = 0.4$. These jets are required to have transverse momentum $p_{\text{T}} > 25$ GeV.

        The following \verb|MadGraph| scripts generate Monte Carlo samples for each production channel.
        \paragraph{GGF Higgs Sample Generation}
        \begin{lstlisting}
            generate p p > h QCD<=99 [QCD]
            output GGF_Higgs
            launch GGF_Higgs

            shower=Pythia8
            detector=Delphes
            analysis=OFF
            madspin=OFF
            done

            set run_card nevents 100000
            set run_card ebeam1 7000.0
            set run_card ebeam2 7000.0

            set run_card use_syst False

            set pythia8_card 25:onMode = off
            set pythia8_card 25:onIfMatch = 22 22
            done
        \end{lstlisting}
        \paragraph{VBF Higgs Sample Generation}
        \begin{lstlisting}
            define v = w+ w- z
            generate p p > h j j $$v
            output VBF_Higgs
            launch VBF_Higgs

            shower=Pythia8
            detector=Delphes
            analysis=OFF
            madspin=OFF
            done

            set run_card nevents 100000
            set run_card ebeam1 7000.0
            set run_card ebeam2 7000.0

            set run_card use_syst False

            set pythia8_card 25:onMode = off
            set pythia8_card 25:onIfMatch = 22 22
            done
        \end{lstlisting}
    % subsection monte_carlo_samples (end)
    \subsection{Event selection}% (fold)
    \label{sub:event_selection}
        The selection cuts after the \verb|Delphes| simulation:
        \begin{itemize}
            \item $n_{\gamma}$ cut: The number of photons should be at least 2.
            \item $n_{j}$ cut: The number of jets should be at least 2.
            \item $m_{\gamma\gamma}$ cut: The invariant mass of two leading photons $m_{\gamma\gamma}$ are required $\text{120 GeV} \le m_{\gamma\gamma} \le \text{130 GeV}$.
        \end{itemize}

        Table~\ref{tab:GGF_VBF_Higgs_cutflow_number} summarizes the cutflow number at different selection cuts.
        \begin{table}[htpb]
            \centering
            \caption{Number of passing events and passing rates for GGF and VBF Higgs production at different selection cuts.}
            \label{tab:GGF_VBF_Higgs_cutflow_number}
            \begin{tabular}{l|rr|rr}
                Cut                    & GGF    & pass rate & VBF    & pass rate \\ \hline
                Total                  & 100000 & 1         & 100000 & 1         \\
                $n_{\gamma}$ cut       & 48286  & 0.48      & 53087  & 0.53      \\
                $n_j$ cut              & 9302   & 0.09      & 42860  & 0.43      \\
                $m_{\gamma\gamma}$ cut & 8864   & 0.09      & 40694  & 0.41     
            \end{tabular}
        \end{table}
        
        Figure~\ref{fig:mjj_deta_distribution} shows the distributions of $m_{jj}$ (the invariant mass of the two leading jets) and $\Delta\eta_{jj}$ (the pseudorapidity difference between the two leading jets). The scatter plot of $m_{jj}$ versus $\Delta\eta_{jj}$ is presented in Figure~\ref{fig:mjj_deta_scatter}.
        \begin{figure}[htpb]
            \centering
            \subfloat[$m_{jj}$ distribution]{
                \includegraphics[width=0.45\textwidth]{mjj_distribution.pdf}
            }
            \subfloat[$\Delta\eta_{jj}$ distribution]{
                \includegraphics[width=0.45\textwidth]{deta_distribution.pdf}
            }
            \caption{Distributions of the invariant mass $m_{jj}$ and pseudorapidity difference $\Delta\eta_{jj}$ of the two leading jets. Red dashed lines are selection cuts used to construct mixed datasets.}
            \label{fig:mjj_deta_distribution}
        \end{figure}
        \begin{figure}[htpb]
            \centering
            \includegraphics[width=0.45\textwidth]{mjj_deta_scatter_plot.pdf}
            \caption{Scatter plot of $m_{jj}$ versus $\Delta\eta_{jj}$. Red dashed lines are selection cuts used to construct mixed datasets.}
            \label{fig:mjj_deta_scatter}
        \end{figure}
    % subsection event_selection (end)
    \subsection{Event image}% (fold)
    \label{sub:event_image}
        The inputs for the neural networks are event images~\cite{Kasieczka:2019dbj,deOliveira:2015xxd, Kasieczka2017nv}. These images are constructed from events that pass the kinematic selection criteria described in section~\ref{sub:event_selection}. Each event image has three channels corresponding to calorimeter towers, tracks, and photons. The following preprocessing steps are applied to all event constituents:
        \begin{enumerate}
            \item Translation: Compute the $p_{\text{T}}$-weighted center in the $\phi$ coordinates, then shift this point to the origin.
            \item Flipping: Flip the highest $p_{\text{T}}$ quadrant to the first quadrant.
            \item Pixelation: Pixelate in a $\eta \in [-5, 5],\ \phi \in [-\pi, \pi]$ box, with $40 \times 40$ pixels 
        \end{enumerate}

        Figure~\ref{fig:GGF_VEF_event_image} shows the event images for GGF and VBF production modes.
        \begin{figure}[htpb]
            \centering
            \subfloat[GGF: Calorimeter Tower]{
                \includegraphics[width=0.45\textwidth]{event_image_GGF-tower.pdf}
            }
            \subfloat[VBF: Calorimeter Tower]{
                \includegraphics[width=0.45\textwidth]{event_image_VBF-tower.pdf}
            } \\
            \subfloat[GGF: Track]{
                \includegraphics[width=0.45\textwidth]{event_image_GGF-track.pdf}
            }
            \subfloat[VBF: Track]{
                \includegraphics[width=0.45\textwidth]{event_image_VBF-track.pdf}
            } \\
            \subfloat[GGF: Photon]{
                \includegraphics[width=0.45\textwidth]{event_image_GGF-photon.pdf}
            }
            \subfloat[VBF: Photon]{
                \includegraphics[width=0.45\textwidth]{event_image_VBF-photon.pdf}
            }
            \caption{Event images for GGF and VBF production, separately shown for calorimeter towers, tracks, and photons.}
            \label{fig:GGF_VEF_event_image}
        \end{figure}
    % subsection event_image (end)
    \subsection{Mixed datasets}% (fold)
    \label{sub:mixed_datasets}
        Based on figure~\ref{fig:mjj_deta_distribution}, we set selection cuts of $m_{jj} > 300$ GeV and $\Delta\eta_{jj} > 3.1$. We consider three cases: applying each cut individually and simultaneously. These cuts define the signal region (SR), which is VBF-like, and the background region (BR), which is GGF-like. Table~\ref{tab:GGF_VBF_Higgs_cutflow_number_mixed_dataset} summarizes the cutflow results for different selection criteria.
        \begin{table}[htpb]
            \centering
            \caption{Number of passing events and passing rates for GGF and VBF Higgs production under different selection cuts.}
            \label{tab:GGF_VBF_Higgs_cutflow_number_mixed_dataset}
            \begin{tabular}{l|rr|rr}
                Cut                                & GGF    & pass rate & VBF    & pass rate \\ \hline
                Total                              & 100000 & 1.00      & 100000 & 1.00      \\
                $n_{\gamma}$ cut                   & 9302   & 0.09      & 42860  & 0.43      \\
                $n_j$ cut                          & 9302   & 0.09      & 42860  & 0.43      \\
                $m_{\gamma\gamma}$ cut             & 8864   & 0.09      & 40694  & 0.41      \\ \hline
                $m_{jj}$ cut: SR                   & 2695   & 0.03      & 29496  & 0.29      \\
                $m_{jj}$ cut: BR                   & 6169   & 0.06      & 11198  & 0.11      \\ \hline
                $\Delta\eta_{jj}$ cut: SR          & 2317   & 0.02      & 28160  & 0.28      \\
                $\Delta\eta_{jj}$ cut: BR          & 6547   & 0.07      & 12534  & 0.13      \\ \hline
                $m_{jj}, \Delta\eta_{jj}$ cuts: SR & 1832   & 0.02      & 26446  & 0.26      \\
                $m_{jj}, \Delta\eta_{jj}$ cuts: BR & 5684   & 0.06      & 9484   & 0.09     
            \end{tabular}
        \end{table}

        The total cross-section for VBF production is $\sigma_{\text{VBF}} = 4.278$ pb$^{-1}$ at NNLO and for GGF production is $\sigma_{\text{GGF}} = 54.67$ pb$^{-1}$ at N3LO, as referenced in \href{https://twiki.cern.ch/twiki/bin/view/LHCPhysics/CERNYellowReportPageAt14TeV}{this link}. The branching ratio for the di-photon decay channel is $\Gamma\left( h \to \gamma\gamma \right) = 2.270 \times 10^{-3}$, as given in \href{https://twiki.cern.ch/twiki/bin/view/LHCPhysics/CERNYellowReportPageBR}{this link}.

        Assuming the luminosity of $\mathcal{L} = \text{300 fb}^{-1}$, we can estimate the number of events belonging to the SR and BR. These results are summarized in table~\ref{tab:number_of_event_in_mixed_dataset}.
        \begin{table}[htpb]
            \centering
            \caption{The number of events of mixed datasets under different selection cuts.}
            \label{tab:number_of_event_in_mixed_dataset}
            \subfloat[$m_{jj} > 300$ GeV]{
                \begin{tabular}{c|cc}
                       & GGF & VBF \\ \hline
                    BR & 2297 & 326 \\
                    SR & 1003 & 859
                \end{tabular}
            }
            \subfloat[$\Delta\eta_{jj} > 3.1$]{
                \begin{tabular}{c|cc}
                       & GGF & VBF \\ \hline
                    BR & 2437 & 365 \\
                    SR & 863  & 820
                \end{tabular}
            } \\
            \subfloat[$m_{jj} > 300$ GeV, \newline $\Delta\eta_{jj} > 3.1$]{
                \begin{tabular}{c|cc}
                       & GGF & VBF \\ \hline
                    BR & 2116 & 276 \\
                    SR & 682  & 770
                \end{tabular}
            }
        \end{table}
    % subsection mixed_datasets (end)
% section sample_preparation (end)
\section{Training CNN}% (fold)
\label{sec:training_cnn}
    The total sample sizes are mentioned in section~\ref{sub:mixed_datasets}. We allocate 80\% of the data for training and 20\% for validation. The testing set consists of the SR's 10,000 VBF and 10,000 GGF events.

    The convolutional neural network (CNN) model structure is summarized in figure~\ref{fig:cnn_model_structure}. The internal node uses the rectified linear unit (ReLU) as the activation function. The loss function is the binary cross-entropy. The \verb|Adam| optimizer minimizes the loss value. The learning rate is $10^{-4}$, and the batch size is 512. We employ the early stopping technique to prevent over-training issues with patience of 10.
    \begin{figure}[htpb]
        \centering
        \begin{tikzpicture}[
            layer/.style={draw, rounded corners, minimum width=2.5cm, text centered},
            arrow/.style={-stealth, thick},
            node distance=0.25cm,
        ]
        % Input layer
        \node (input) [layer, fill=blue!20,] {Input: $40 \times 40 \times 3$};

        % Jet 1 layers
        \node (bn1) [below=of input, layer, fill=purple!20] {BatchNorm};


        % Conv1_1 layer positioned between bn1 and bn2
        \node (conv1_1) [below=0.25cm of bn1, layer, fill=green!20] {Conv2D 64, $5 \times 5$, MaxPool $2 \times 2$};

        % Other Conv layers
        \node (conv2_1) [below=of conv1_1, layer, fill=green!20] {Conv2D 64, $5 \times 5$, MaxPool $2 \times 2$};
        \node (conv3_1) [below=of conv2_1, layer, fill=green!20] {Conv2D 128, $3 \times 3$, MaxPool $2 \times 2$};
        \node (conv4_1) [below=of conv3_1, layer, fill=green!20] {Conv2D 128, $3 \times 3$};

        % Dense layers
        \node (flatten1) [below=of conv4_1, layer, fill=pink!20] {Flatten};
        \node (dense1_1) [below=of flatten1, layer, fill=red!20] {Dense 128};
        \node (dense2_1) [below=of dense1_1, layer, fill=red!20] {Dense 128};
        \node (dense3_1) [below=of dense2_1, layer, fill=red!20] {Dense 128};
        \node (output1) [below=of dense3_1, layer, fill=red!20] {Dense (Output layer) 1};


        % Arrows for BN
        \draw [arrow] (input) -- (bn1);
        \draw [arrow] (bn1) -- (conv1_1);
        \draw [arrow] (conv1_1) -- (conv2_1);
        \draw [arrow] (conv2_1) -- (conv3_1);
        \draw [arrow] (conv3_1) -- (conv4_1);
        \draw [arrow] (conv4_1) -- (flatten1);
        \draw [arrow] (flatten1) -- (dense1_1);
        \draw [arrow] (dense1_1) -- (dense2_1);
        \draw [arrow] (dense2_1) -- (dense3_1);
        \draw [arrow] (dense3_1) -- (output1);

        \end{tikzpicture}
        \caption{The architecture of the CNN model with key hyperparameters.}
        \label{fig:cnn_model_structure}
    \end{figure}

    The training results are summarized in table~\ref{tab:CWoLa_CNN_training_results}. The performance of the $\Delta\eta_{jj}$ cuts is better than the $m_{jj}$ cut. Moreover, when both cuts are applied together, the performance is slightly worse than when applying either cut individually.
    \begin{table}[htpb]
        \centering
        \caption{The CNN training results. The ACC and AUC are evaluated based on 10 training. The selection cuts of $m_{jj} > \text{300 GeV}$ and $\Delta\eta_{jj} > 3.1$ are applied.}
        \label{tab:CWoLa_CNN_training_results}
        \begin{tabular}{l|cc|cc}
                                      & \multicolumn{2}{c|}{$M_1 / M_2$}      & \multicolumn{2}{c}{$S / B$}           \\ \hline
            Cut                       & ACC               & AUC               & ACC               & AUC               \\ \hline
            $m_{jj}$                  & $0.712 \pm 0.023$ & $0.741 \pm 0.041$ & $0.576 \pm 0.010$ & $0.596 \pm 0.014$ \\
            $\Delta\eta_{jj}$         & $0.828 \pm 0.043$ & $0.889 \pm 0.050$ & $0.604 \pm 0.014$ & $0.630 \pm 0.015$ \\
            $m_{jj}, \Delta\eta_{jj}$ & $0.753 \pm 0.022$ & $0.792 \pm 0.035$ & $0.573 \pm 0.007$ & $0.596 \pm 0.008$
        \end{tabular}
    \end{table}
    % subsection training_cnn (end)
    \subsection{More events}% (fold)
    \label{sub:more_events}
        This section assumes the luminosity of $\mathcal{L} = \text{3000 fb}^{-1}$. The number of events belonging to the SR and BR are summarized in table~\ref{tab:number_of_event_in_mixed_dataset_3000}.
        \begin{table}[htpb]
            \centering
            \caption{The number of events of mixed datasets under different selection cuts.}
            \label{tab:number_of_event_in_mixed_dataset_3000}
            \subfloat[$m_{jj} > 300$ GeV]{
                \begin{tabular}{c|cc}
                       & GGF & VBF \\ \hline
                    BR & 22967 & 3262 \\
                    SR & 10034 & 8593
                \end{tabular}
            }
            \subfloat[$\Delta\eta_{jj} > 3.1$]{
                \begin{tabular}{c|cc}
                       & GGF & VBF \\ \hline
                    BR & 24375 & 3652 \\
                    SR & 8626  & 8204
                \end{tabular}
            } \\
            \subfloat[$m_{jj} > 300$ GeV, \newline $\Delta\eta_{jj} > 3.1$]{
                \begin{tabular}{c|cc}
                       & GGF & VBF \\ \hline
                    BR & 21162 & 2763 \\
                    SR & 6821  & 7705
                \end{tabular}
            }
        \end{table}

        The training results are summarized in table~\ref{tab:CWoLa_CNN_training_results_3000}. All datasets' performance is better than the results in table~\ref{tab:CWoLa_CNN_training_results}. The $\Delta\eta_{jj}$ cut performs better than the $m_{jj}$ cut. Moreover, when both cuts are applied together, the performance is slightly worse than the $\Delta\eta_{jj}$ cut but better than $m_{jj}$. These results are similar to the previous one. 
        \begin{table}[htpb]
            \centering
            \caption{The CNN training results. The ACC and AUC are evaluated based on 10 training. The selection cuts of $m_{jj} > \text{300 GeV}$ and $\Delta\eta_{jj} > 3.1$ are applied.}
            \label{tab:CWoLa_CNN_training_results_3000}
            \begin{tabular}{l|cc|cc}
                                          & \multicolumn{2}{c|}{$M_1 / M_2$}      & \multicolumn{2}{c}{$S / B$}           \\ \hline
                Cut                       & ACC               & AUC               & ACC               & AUC               \\ \hline
                $m_{jj}$                  & $0.907 \pm 0.002$ & $0.969 \pm 0.002$ & $0.598 \pm 0.008$ & $0.625 \pm 0.009$ \\
                $\Delta\eta_{jj}$         & $0.931 \pm 0.004$ & $0.979 \pm 0.002$ & $0.615 \pm 0.005$ & $0.648 \pm 0.006$ \\
                $m_{jj}, \Delta\eta_{jj}$ & $0.929 \pm 0.003$ & $0.978 \pm 0.002$ & $0.608 \pm 0.004$ & $0.638 \pm 0.005$
            \end{tabular}
        \end{table}
    % subsection more_events (end)
% section training_cnn (end)
\section{\texorpdfstring{$p_{\mathrm{T}}$}{pT} normalization}% (fold)
\label{sec:pt_normalization}
    
    To remove the potential dependence of the input samples on $m_{jj}$, we standardize the event images to remove the difference in input data distributions between the SR and BR. We calculate the mean and standard deviation of the event image transverse momentum and use these values to standardize each event image. We standardize each channel separately.
    
    The number of events in the SR and BR are the same as previously in table~\ref{tab:number_of_event_in_mixed_dataset_3000}.

    The training results are summarized in table~\ref{tab:CWoLa_CNN_training_results_3000_pT_norm}. The $m_{jj}$ cut performs better than the previous one (table~\ref{tab:CWoLa_CNN_training_results_3000}). 
    \begin{table}[htpb]
        \centering
        \caption{The CNN training results with $p_{\text{T}}$ normalization technique. The ACC and AUC are evaluated based on 10 training. The selection cuts of $m_{jj} > \text{300 GeV}$ and $\Delta\eta_{jj} > 3.1$ are applied. }
        \label{tab:CWoLa_CNN_training_results_3000_pT_norm}
        \begin{tabular}{l|cc|cc}
                                      & \multicolumn{2}{c|}{$M_1 / M_2$}      & \multicolumn{2}{c}{$S / B$}           \\ \hline
            Cut                       & ACC               & AUC               & ACC               & AUC               \\ \hline
            $m_{jj}$                  & $0.874 \pm 0.004$ & $0.946 \pm 0.003$ & $0.624 \pm 0.005$ & $0.663 \pm 0.006$ \\
            $\Delta\eta_{jj}$         & $0.928 \pm 0.005$ & $0.979 \pm 0.002$ & $0.597 \pm 0.005$ & $0.630 \pm 0.006$ \\
            $m_{jj}, \Delta\eta_{jj}$ & $0.917 \pm 0.003$ & $0.973 \pm 0.002$ & $0.603 \pm 0.004$ & $0.636 \pm 0.006$
        \end{tabular}
    \end{table}

% section pt_normalization (end)
\section{Different cut setting}% (fold)
\label{sec:different_cut_setting}
    
    We set selection cuts of $m_{jj} > 225$ GeV and $\Delta\eta_{jj} > 2.3$ to ensure the SR and BR datasets have similar sizes. Table~\ref{tab:GGF_VBF_Higgs_cutflow_number_mixed_dataset_225_2.3} summarizes the cutflow results for different selection criteria.
    \begin{table}[htpb]
        \centering
        \caption{Number of passing events and passing rates for GGF and VBF Higgs production under different selection cuts.}
        \label{tab:GGF_VBF_Higgs_cutflow_number_mixed_dataset_225_2.3}
        \begin{tabular}{l|rr|rr}
            Cut                                & GGF    & pass rate & VBF    & pass rate \\ \hline
            Total                              & 100000 & 1.00      & 100000 & 1.00      \\
            $n_{\gamma}$ cut                   & 9302   & 0.09      & 42860  & 0.43      \\
            $n_j$ cut                          & 9302   & 0.09      & 42860  & 0.43      \\
            $m_{\gamma\gamma}$ cut             & 8864   & 0.09      & 40694  & 0.41      \\ \hline
            $m_{jj}$ cut: SR                   & 3638   & 0.04      & 32993  & 0.33      \\
            $m_{jj}$ cut: BR                   & 5226   & 0.05      & 7701   & 0.08      \\ \hline
            $\Delta\eta_{jj}$ cut: SR          & 3611   & 0.04      & 32914  & 0.33      \\
            $\Delta\eta_{jj}$ cut: BR          & 5253   & 0.05      & 7780   & 0.08      \\ \hline
            $m_{jj}, \Delta\eta_{jj}$ cuts: SR & 2842   & 0.03      & 31113  & 0.31      \\
            $m_{jj}, \Delta\eta_{jj}$ cuts: BR & 4457   & 0.04      & 5900   & 0.06     
        \end{tabular}
    \end{table}

    Assuming the luminosity of $\mathcal{L} = \text{3000 fb}^{-1}$, we can estimate the number of events belonging to the SR and BR. These results are summarized in table~\ref{tab:number_of_event_in_mixed_dataset_225_2.3}
    \begin{table}[htpb]
        \centering
        \caption{The number of events of mixed datasets under different selection cuts.}
        \label{tab:number_of_event_in_mixed_dataset_225_2.3}
        \subfloat[$m_{jj} > 225$ GeV]{
            \begin{tabular}{c|cc}
                   & GGF & VBF \\ \hline
                BR & 19457 & 2244 \\
                SR & 13544 & 9612
            \end{tabular}
        }
        \subfloat[$\Delta\eta_{jj} > 2.3$]{
            \begin{tabular}{c|cc}
                   & GGF & VBF \\ \hline
                BR & 19557 & 2267 \\
                SR & 13444 & 9589
            \end{tabular}
        } \\
        \subfloat[$m_{jj} > 225$ GeV, \newline $\Delta\eta_{jj} > 2.3$]{
            \begin{tabular}{c|cc}
                   & GGF & VBF \\ \hline
                BR & 16594 & 1719 \\
                SR & 10581 & 9064
            \end{tabular}
        }
    \end{table}

    The training results are summarized in table~\ref{tab:CWoLa_CNN_training_results_3000_pT_norm_225_2.3}. The results are better than the table~\ref{tab:CWoLa_CNN_training_results_3000_pT_norm} by 1\%. Similarly, the $m_{jj}$ cut performs best. 
    \begin{table}[htpb]
        \centering
        \caption{The CNN training results with $p_{\text{T}}$ normalization technique. The ACC and AUC are evaluated based on 10 training. The selection cuts of $m_{jj} > \text{225 GeV}$ and $\Delta\eta_{jj} > 2.3$ are applied.}
        \label{tab:CWoLa_CNN_training_results_3000_pT_norm_225_2.3}
        \begin{tabular}{l|cc|cc}
                                      & \multicolumn{2}{c|}{$M_1 / M_2$}      & \multicolumn{2}{c}{$S / B$}           \\ \hline
            Cut                       & ACC               & AUC               & ACC               & AUC               \\ \hline
            $m_{jj}$                  & $0.864 \pm 0.004$ & $0.940 \pm 0.004$ & $0.632 \pm 0.006$ & $0.673 \pm 0.007$ \\
            $\Delta\eta_{jj}$         & $0.913 \pm 0.006$ & $0.972 \pm 0.003$ & $0.605 \pm 0.007$ & $0.640 \pm 0.009$ \\
            $m_{jj}, \Delta\eta_{jj}$ & $0.896 \pm 0.007$ & $0.961 \pm 0.004$ & $0.616 \pm 0.005$ & $0.653 \pm 0.006$
        \end{tabular}
    \end{table}
% section different_cut_setting (end)
\section{Supervised training}% (fold)
\label{sec:supervised_training}
    This section tests the supervised training on CNN. The training, validation, and testing sample size are summarized in table~\ref{tab:supervised_sample_size}. The events passing all selection requirements (section~\ref{sub:event_selection}) are considered.
    \begin{table}[htbp]
        \centering
        \caption{Sizes of various samples used for supervised training.}
        \label{tab:supervised_sample_size}
        \begin{tabular}{l|ccc}
                & Training & Validation & Testing \\ \hline
            GGF & 100k     & 25k        & 25k     \\
            VBF & 100k     & 25k        & 25k      
        \end{tabular}
    \end{table}

    The training results are summarized in table~\ref{tab:supervised_CNN_training_results}. These results demonstrate the upper limit of CNN training.
    \begin{table}[htpb]
        \centering
        \caption{The CNN training results with $p_{\text{T}}$ normalization technique. The ACC and AUC are evaluated based on 10 training.}
        \label{tab:supervised_CNN_training_results}
        \begin{tabular}{c|c}
             ACC               & AUC               \\ \hline
             $0.784 \pm 0.001$ & $0.861 \pm 0.001$ \\
        \end{tabular}
    \end{table}

    \subsection{Testing sample in SR and BR}% (fold)
    \label{sub:testing_sample_in_sr_and_br}
        The testing events used to evaluate the table~\ref{tab:supervised_CNN_training_results} are all events passing the selection and not restricted to the particular SR. Thus, to make a fair comparison with previous results, we must evaluate the training performance on the events in SR and BR.

        The new testing dataset consists of the 10,000 VBF and 10,000 GGF events from SR and BR. The number of SR and BR events are computed from table~\ref{tab:GGF_VBF_Higgs_cutflow_number_mixed_dataset_225_2.3}.

        The training results of table~\ref{tab:CWoLa_CNN_training_results_3000_pT_norm_225_2.3} are re-evaluated on the new testing set and shown in table~\ref{tab:CWoLa_CNN_training_results_3000_pT_norm_225_2.3_new_test_set}. The results are better than the table~\ref{tab:CWoLa_CNN_training_results_3000_pT_norm_225_2.3}. It seems that the events in the BR can be distinguished better than those in SR.
    \begin{table}[htpb]
        \centering
        \caption{The CNN training results with $p_{\text{T}}$ normalization technique. The ACC and AUC are evaluated based on 10 training. The selection cuts of $m_{jj} > \text{225 GeV}$ and $\Delta\eta_{jj} > 2.3$ are applied.}
        \label{tab:CWoLa_CNN_training_results_3000_pT_norm_225_2.3_new_test_set}
        \begin{tabular}{l|cc|cc}
                                      & \multicolumn{2}{c|}{$M_1 / M_2$}      & \multicolumn{2}{c}{$S / B$}           \\ \hline
            Cut                       & ACC               & AUC               & ACC               & AUC               \\ \hline
            $m_{jj}$                  & $0.863 \pm 0.004$ & $0.940 \pm 0.002$ & $0.716 \pm 0.003$ & $0.780 \pm 0.004$ \\
            $\Delta\eta_{jj}$         & $0.914 \pm 0.004$ & $0.972 \pm 0.003$ & $0.702 \pm 0.003$ & $0.754 \pm 0.003$ \\
            $m_{jj}, \Delta\eta_{jj}$ & $0.896 \pm 0.006$ & $0.962 \pm 0.004$ & $0.723 \pm 0.003$ & $0.780 \pm 0.002$
        \end{tabular}
    \end{table}
    % subsection testing_sample_in_sr_and_br (end)
% section supervised_training (end)
\section{Use jet tagging results to construct mixed datasets}% (fold)
\label{sec:use_jet_tagging_results_to_construct_mixed_datasets}
    This section uses the jet tagging results to construct the mixed datasets.

    Assuming the luminosity of $\mathcal{L} = \text{3000 fb}^{-1}$, we can estimate the number of events belonging to the SR and BR. The SR and BR are defined based on the number of the gluon jets $n_g$ and quark jets $n_q$. The selection results are summarized in table~\ref{tab:number_of_event_in_mixed_dataset_gluon_jet}.

    \begin{table}[htpb]
        \centering
        \caption{The number of events of mixed datasets under different selection cuts. Here, $agbq$ means that $n_g=a, n_q=b$.}
        \label{tab:number_of_event_in_mixed_dataset_gluon_jet}
        \subfloat[SR: $2q0g$; \\ BR: $1q1g, 0q2g$]{
            \begin{tabular}{c|cc}
                   & GGF   & VBF     \\ \hline
                SR & 16828 & 10229   \\
                BR & 16865 & 1596    \\
            \end{tabular}
        }
        \subfloat[SR: $2q0g, 1q1g$; \\ BR: $0q2g$]{
            \begin{tabular}{c|cc}
                   & GGF   & VBF   \\ \hline
                SR & 30752 & 11779 \\
                BR & 2941  & 47    \\
            \end{tabular}
        }
        \subfloat[SR: $2q0g$; BR: $0q2g$]{
            \begin{tabular}{c|cc}
                   & GGF   & VBF     \\ \hline
                SR & 16828 & 10229   \\
                BR & 2941  & 47      \\
            \end{tabular}
        }
    \end{table}

    For now, we use the truth information from \verb|Delphes| and do not consider the mis-tagging case.

    The training results are summarized in table~\ref{tab:CWoLa_CNN_training_results_3000_pT_norm_jet_tagging}. All different jet-tagging conditions produced similar performance. However, the results are worse than the ones of kinematic cuts (table~\ref{tab:CWoLa_CNN_training_results_3000_pT_norm_225_2.3_new_test_set}).
    \begin{table}[htpb]
        \centering
        \caption{The CNN training results with $p_{\text{T}}$ normalization technique. The ACC and AUC are evaluated based on 10 training. The selection cuts of the number of gluon jets are applied.}
        \label{tab:CWoLa_CNN_training_results_3000_pT_norm_jet_tagging}
        \begin{tabular}{l|cc|cc}
                                         & \multicolumn{2}{c|}{$M_1 / M_2$}      & \multicolumn{2}{c}{$S / B$}           \\ \hline
            Datasets                     & ACC               & AUC               & ACC               & AUC               \\ \hline
            SR: $2q0g$; BR: $1q1g, 0q2g$ & $0.623 \pm 0.005$ & $0.642 \pm 0.005$ & $0.653 \pm 0.008$ & $0.706 \pm 0.009$ \\
            SR: $2q0g, 1q1g$; BR: $0q2g$ & $0.934 \pm 0.000$ & $0.689 \pm 0.012$ & $0.662 \pm 0.006$ & $0.719 \pm 0.008$ \\
            SR: $2q0g$; BR: $0q2g$       & $0.900 \pm 0.000$ & $0.740 \pm 0.010$ & $0.655 \pm 0.008$ & $0.710 \pm 0.009$
        \end{tabular}
    \end{table}

    The training results without $p_{\text{T}}$ nomalization are summarized in table~\ref{tab:CWoLa_CNN_training_results_3000_jet_tagging}. All different jet-tagging conditions produced similar performance. However, the results are worse than the ones with $p_{\text{T}}$ normalization (table~\ref{tab:CWoLa_CNN_training_results_3000_pT_norm_jet_tagging}) by 2\%.
    \begin{table}[htpb]
        \centering
        \caption{The CNN training results without $p_{\text{T}}$ normalization technique. The ACC and AUC are evaluated based on 10 training. The selection cuts of the number of gluon jets are applied.}
        \label{tab:CWoLa_CNN_training_results_3000_jet_tagging}
        \begin{tabular}{l|cc|cc}
                                         & \multicolumn{2}{c|}{$M_1 / M_2$}      & \multicolumn{2}{c}{$S / B$}           \\ \hline
            Datasets                     & ACC               & AUC               & ACC               & AUC               \\ \hline
            SR: $2q0g$; BR: $1q1g, 0q2g$ & $0.614 \pm 0.007$ & $0.632 \pm 0.011$ & $0.646 \pm 0.008$ & $0.690 \pm 0.011$ \\
            SR: $2q0g, 1q1g$; BR: $0q2g$ & $0.934 \pm 0.000$ & $0.695 \pm 0.015$ & $0.643 \pm 0.009$ & $0.689 \pm 0.011$ \\
            SR: $2q0g$; BR: $0q2g$       & $0.900 \pm 0.000$ & $0.743 \pm 0.011$ & $0.632 \pm 0.007$ & $0.677 \pm 0.008$
        \end{tabular}
    \end{table}

    \subsection{Loss weighted}% (fold)
    \label{sub:loss_weighted}
        Since the sample sizes are unbablanced, we add the class weighted. The weights are propotional to the reciprocal of number of events.

        The training results with class weighted are summarized in table~\ref{tab:CWoLa_CNN_training_results_3000_jet_tagging_weighted_loss}. All different jet-tagging conditions produced similar performance.
    \begin{table}[htpb]
        \centering
        \caption{The CNN training results without $p_{\text{T}}$ normalization technique. The ACC and AUC are evaluated based on 10 training. The selection cuts of the number of gluon jets are applied.}
        \label{tab:CWoLa_CNN_training_results_3000_jet_tagging_weighted_loss}
        \begin{tabular}{l|cc|cc}
                                         & \multicolumn{2}{c|}{$M_1 / M_2$}      & \multicolumn{2}{c}{$S / B$}           \\ \hline
            Datasets                     & ACC               & AUC               & ACC               & AUC               \\ \hline
            SR: $2q0g$; BR: $1q1g, 0q2g$ & $0.621 \pm 0.006$ & $0.635 \pm 0.007$ & $0.645 \pm 0.009$ & $0.688 \pm 0.013$ \\
            SR: $2q0g, 1q1g$; BR: $0q2g$ & $0.934 \pm 0.000$ & $0.679 \pm 0.016$ & $0.624 \pm 0.005$ & $0.662 \pm 0.008$ \\
            SR: $2q0g$; BR: $0q2g$       & $0.900 \pm 0.000$ & $0.730 \pm 0.013$ & $0.621 \pm 0.005$ & $0.658 \pm 0.008$
        \end{tabular}
    \end{table}
    % subsection loss_weighted (end)
% section use_jet_tagging_results_to_construct_mixed_datasets (end)
\section{Total scaling of transverse momentum}% (fold)
\label{sec:total_scaling_of_transverse_momentum}
    The $p_{\text{T}}$ normalization remove the magnitude information of the input datasets. Thus, we would expect the training performance of the $p_{\text{T}}$ normalization datasets would be worse than the one without it. However, table~\ref{tab:CWoLa_CNN_training_results_3000_pT_norm_jet_tagging} and \ref{tab:CWoLa_CNN_training_results_3000_jet_tagging} shows the opposite results.

    To explore the reason why the $p_\text{T}$ normalization could improve the training performance, we try the total $p_{\text{T}}$ scaling, which computes the mean and statard deviation of all input samples. Then, use these values to standardize the input datasets.

    \subsection{Results}% (fold)
    \label{sub:results}
        The training results with $p_{\text{T}}$ scaling are summarized in table~\ref{tab:CWoLa_CNN_training_results_3000_jet_tagging_pT_scaling}. All different jet-tagging conditions produced similar performance. However, the results are worse than the ones with $p_{\text{T}}$ normalization (table~\ref{tab:CWoLa_CNN_training_results_3000_pT_norm_jet_tagging}).
        \begin{table}[htpb]
            \centering
            \caption{The CNN training results with $p_{\text{T}}$ scaling technique. The ACC and AUC are evaluated based on 10 training. The selection cuts of the number of gluon jets are applied.}
            \label{tab:CWoLa_CNN_training_results_3000_jet_tagging_pT_scaling}
            \begin{tabular}{l|cc|cc}
                                             & \multicolumn{2}{c|}{$M_1 / M_2$}      & \multicolumn{2}{c}{$S / B$}           \\ \hline
                Datasets                     & ACC               & AUC               & ACC               & AUC               \\ \hline
                SR: $2q0g$; BR: $1q1g, 0q2g$ & $0.622 \pm 0.004$ & $0.637 \pm 0.008$ & $0.638 \pm 0.009$ & $0.678 \pm 0.011$ \\
                SR: $2q0g, 1q1g$; BR: $0q2g$ & $0.934 \pm 0.000$ & $0.673 \pm 0.032$ & $0.619 \pm 0.019$ & $0.652 \pm 0.029$ \\
                SR: $2q0g$; BR: $0q2g$       & $0.900 \pm 0.000$ & $0.733 \pm 0.011$ & $0.621 \pm 0.006$ & $0.657 \pm 0.009$
            \end{tabular}
        \end{table}

        The training results with $p_{\text{T}}$ normalization are summarized in table~\ref{tab:CWoLa_CNN_training_results_3000_jet_tagging_w_pT_norm}. 
        \begin{table}[htpb]
            \centering
            \caption{The CNN training results with $p_{\text{T}}$ normalization technique. The ACC and AUC are evaluated based on 10 training. The selection cuts of the number of gluon jets are applied.}
            \label{tab:CWoLa_CNN_training_results_3000_jet_tagging_w_pT_norm}
            \begin{tabular}{l|cc|cc}
                                             & \multicolumn{2}{c|}{$M_1 / M_2$}      & \multicolumn{2}{c}{$S / B$}           \\ \hline
                Datasets                     & ACC               & AUC               & ACC               & AUC               \\ \hline
                SR: $2q0g$; BR: $1q1g, 0q2g$ & $0.615 \pm 0.005$ & $0.632 \pm 0.007$ & $0.650 \pm 0.011$ & $0.703 \pm 0.015$ \\
                SR: $2q0g, 1q1g$; BR: $0q2g$ & $0.934 \pm 0.000$ & $0.662 \pm 0.014$ & $0.630 \pm 0.008$ & $0.675 \pm 0.011$ \\
                SR: $2q0g$; BR: $0q2g$       & $0.900 \pm 0.000$ & $0.716 \pm 0.012$ & $0.640 \pm 0.007$ & $0.690 \pm 0.009$
            \end{tabular}
        \end{table}

        The training results without $p_{\text{T}}$ normalization are summarized in table~\ref{tab:CWoLa_CNN_training_results_3000_jet_tagging_wo_pT_norm}. 
        \begin{table}[htpb]
            \centering
            \caption{The CNN training results without $p_{\text{T}}$ normalization technique. The ACC and AUC are evaluated based on 10 training. The selection cuts of the number of gluon jets are applied.}
            \label{tab:CWoLa_CNN_training_results_3000_jet_tagging_wo_pT_norm}
            \begin{tabular}{l|cc|cc}
                                             & \multicolumn{2}{c|}{$M_1 / M_2$}      & \multicolumn{2}{c}{$S / B$}           \\ \hline
                Datasets                     & ACC               & AUC               & ACC               & AUC               \\ \hline
                SR: $2q0g$; BR: $1q1g, 0q2g$ & $0.620 \pm 0.004$ & $0.636 \pm 0.005$ & $0.643 \pm 0.006$ & $0.686 \pm 0.007$ \\
                SR: $2q0g, 1q1g$; BR: $0q2g$ & $0.934 \pm 0.000$ & $0.680 \pm 0.014$ & $0.624 \pm 0.010$ & $0.660 \pm 0.016$ \\
                SR: $2q0g$; BR: $0q2g$       & $0.900 \pm 0.000$ & $0.727 \pm 0.010$ & $0.628 \pm 0.008$ & $0.666 \pm 0.011$
            \end{tabular}
        \end{table}

    % subsection results (end)

% section total_scaling_of_transverse_momentum (end)
\bibliographystyle{ieeetr}
\bibliography{reference}

\end{document}
